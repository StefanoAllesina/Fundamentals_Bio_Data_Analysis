---
title: "Week 1: basics of data and its visualization"
author: "Dmitry Kondrashov & Stefano Allesina"
date: "Fundamentals of Biological Data Analysis -- BIOS 26318"
output:
  html_document:
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
urlcolor: blue
---

```{r knitr, echo=FALSE}
knitr::opts_chunk$set(
  eval      = TRUE,
  comment   = "#",
  results   = "hold",
  # collapse  = TRUE,
  fig.align = "center")
library(tidyverse)
```

## Reading:  Chapter 1 of Hadley and Wickham



# Logistics and goals of the course

* R tools for working with data
* Theory to know how to set up a question
* Avoid common mistakes
* Apply this to your own data


# Review of sample spaces and random variables

A random experiment can result in a range of *outcomes* (at least two and possibly infinitely many.) The collection of all outcomes of an experiment is called its *sample space* or *probability space*.

Where does the "random" factor come from? Few things are inherently unpredictable in the physical sense (the exception being quantum phenomena governed by the undertainty principle). Most phenomena are theoretically deterministic for an omniscient being with an unlimited computational power. For macroscopic systems, randomness/unpredictability is usually the result of either complexity (e.g. biomolecular systems, prediction of animal behavior) or of some external noise (e.g. measurement error, weather affecting food availability).

**Example:** The specifics of the experiment can affect whether a variable is random. For example, measuring the height of a person should be deterministic, modulo measurement error, if one measures the height of the same person within a short amount of time. However, measuring the height of different people is a random experiment, where the source of randomness is external (the selection of person) rather than inherent to the person.

The result of a measurement made from a random experiment is called a *random variable*. Sometimes the measurement simply reports the outcome, but usually it reports some aspect of the outcome and so several outcomes can have the same value of the random variable. The random variable can then be seen as condensing the sample space into a smaller range of values (numeric or categorical).

**Example:** In a DNA sequence a codon triplet represents a specific amino acid, but there is redundancy (several triplets may code for the same amino acid). One may think of a coding DNA sequence as an outcome, but the amino acid (sequence or single one) as a random variable. Extending this framework, one may think of genotype as an outcome, but a phenotype (e.g. eye color) as a random variable.


**Exercise:** In the mpg dataset in dplyr, certain variables are numeric and others are categorical; identify them.

# Distributions, means, and variances: probability

An outcome in sample space can be assigned a *probability* depending on its frequency of occurrence out of many trials, each is a number between 0 and 1. Combinations of outcomes (*events*) can be assigned probabilities by building them out of individual outcomes.  These probabilities have a few rules, called the *axioms of probability*:

1. The total probability of all outcomes in sample space is 1. $P(\Omega) = 1$

2. The probability of nothing (empty set) is 0. $P(\emptyset) = 0$

3. The probability of an event made up of the union of two events is the sum of the two probabilities minus the probability of the overlap (intersection.) $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

The probability of each value of a random variable can be calculated from the events that correspond to each value of the random variable. The collection of the probabilities of all of the values of the random variable is called the *probability distribution function* of the random variable, more formally the *mass function* for a discrete random variable or the *density function* for a continuous random variable.

**Example:** To find the probability of a particular amino acid in a collection of codons, first there has to be a probability assigned to every possible three-letter codon. There are $4^3 = 64$ codons, so if one assumes that each one has equal probability $1/64$, then the probability of each amino acid is the number of triplets that code for it divided by 64. For example, the probabilities of leucine and arginine are $6/64 = 3/32$, the probability of threonine is $4/64 = 1/4$ and the probabilities of methionine and tryptophan are $1/64$. Note that the sum of all the probabilites of amino acids has to be 1. Of course there is no inherent reason why each triplet should be equally probable, so a different probability structure on the sample space would result in a different probability distribution (mass) function.

# Data as samples from distributions: statistics

In scientific practice, we collect data from one or more random variables, called a *sample*, and then try to make sense of it. One of the basic goals is statistical inference: using the data set to describe the *population* distribution from the which the sample was drawn. Some of the fundamental questions about the population include:

1. What type of distribution is it?

2. Estimate the parameters of that distribution.

3. Test a hypothesis, e.g. whether two samples were drawn from the same distribution.

4. Describe and test a relationship between two or more variables.

First, the sample has to be *unbiased*, that is, no outcomes should be systematically over- or under-represented. But even an unbiased sample will differ from the population. The **law of large numbers** states that as the *sample size* increases, the mean of the sample converges to the true mean of the population (provided the sample is a collection of independent, identically distributed random variables.)

That is nice to know, but doesn't say exactly how large a sample is needed to estimate, for example, the mean of the population to a given precision. For that, we have the **Central Limit Theorem**, which states that the distribution of sample means (from samples of independent, identically distributed random variables) as sample size increases, approaches the normal (Gaussian) distribution with mean equal to the population mean and standard deviation equal to the standard deviation of the population divided by the square root of the sample size. This is an amazing result because it applies to any distribution, so it allows for the estimation of means for any situation, as long as the condition of indepdendent, identically disributed variables in the sample is satisfied. There are other central limit theorems that apply to more specific situations, including cases where the random variables in the sample are not independent (e.g. Markov models.)

Essentially, an unbiased sample contains a reflection of the true population, but it is always distorted by uncertainty. Larger sample sizes decrease the uncertainty, but are more difficult and expensive to obtain.

**Discussion:** Come up with examples of biological data sets which are not made up of independent identically distributed random variables.

# Over to the students

1. Students introduce their own data sets


2. Describe variables and observations, data types


3. Solicit suggestions for questions that they would like to address using their data sets

# Examples of mistakes and surprising data sets

Simpson's paradox (misleading means):

https://medium.com/@nikhilborkar/the-simpsons-paradox-and-where-to-find-them-cfcec6c2d8b3

Use the library titanic and combine the data sets for all passangers and crew into the following tibble:
```{r}
library(titanic)
titanic_total <- bind_rows(titanic_test, titanic_train)
```

First, calculate the probability of survival for passengers by class (1, 2, 3, and crew). Then calculate the same by sex and. Then compare the survival rates of men and women separately by class. Do you observe  anything unexpected? How would you explain the apparent disagreement between the survival rates?

