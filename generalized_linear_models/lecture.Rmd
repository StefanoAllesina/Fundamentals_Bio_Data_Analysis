---
title: "Generalized Linear models"
author: "Dmitry Kondrashov & Stefano Allesina"
date: "Fundamentals of Biological Data Analysis -- BIOS 26318"
output:
  html_document:
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
urlcolor: blue
---

```{r knitr, echo=FALSE}
knitr::opts_chunk$set(
  eval      = TRUE,
  comment   = "#",
  warning = FALSE,
  message = FALSE,
  #results   = "asis",
  # collapse  = TRUE,
  fig.align = "center")
```

# Goal
Learn about Generalized Linear Models (GLMs), and be able to decide which model is most appropriate for the problem at hand.

Let's load some packages:
```{r}
library(MASS) # negative binom regression
library(tidyverse) # our friend the tidyverse
library(pscl) # zero-inflated and zero-hurdle models
source("general_code/read_xls_from_url.R") # function to read excel from URL
```

# Beyond linear regression

The basic framework of linear regression has been extended in several directions. For example, 

- [*Measurement error models*](https://en.wikipedia.org/wiki/Errors-in-variables_models) are used when the independent variables are measured with errors (i.e., are not known precisely).

- Other types here...


# Generalized linear models (GLMs)

The linear regression we've explored during the past week attempts to estimate the expected value for *response* (dependent) variable $Y$ given the *predictors* $X$. It assumes that the response variable changes continuously, and that errors are normally distributed around the mean. In many cases, however, the response variable does not have support in the whole real line, or errors are not normally distributed. In these cases, one can use Generalized Linear Models to fit the data.

In GLMs,

- The response variable is modeled by a single-parameter distribution from the exponential family (Gaussian, Gamma, Binomial, Poisson, etc.)
- A *link function* linearizes the relationship between the fitted value and the predictors.
- Parameters are estimated through a least squares algorithm.


## Model structure

In practice, we need to determine three parts of the model:

- *Random component* the entries of the response variable ($Y$) are assumed to be independently drawn from a certain distribution (e.g., Binomial).

- *Systematic component* the explanatory variables ($X_1$, $X_2$, $\ldots$) are combined in a linear combination to form a *linear predictor* (e.g., $\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots$). The explanatory variables can be continuous, categorical, or mixed.

- *Link function* $g(u)$ specifies how the random and systematic components are connected. 

# Binary data

The most extreme case of departure from normality is when the response variable can assume only values 0 or 1 (no/yes, survived/deceased, lost/won, etc.). A Bernoulli random variable can take values 0 or 1, and therefore provides the *Random component* of the model:

$$
P(Y_i = y_i | \pi_i) = \pi_i^{y_i} (1 - \pi_i)^{1 - y_i}
$$

Now we want to relate the parameter $\pi_i$ to the *linear predictor*. This can be accomplished in a number of ways.

## Logistic regression

The most popular choice is to use the *Logit* function as the link function:

$$
\text{Logit}(\pi_i) = \beta_0 + \beta_1 x_i 
$$

where the function can be written as:

$$
\text{Logit}(\pi_i) = \log\left( \frac{\pi_i}{1 - \pi_i} \right) = \log(\pi_i) - \log(1 - \pi_i)
$$
This means that 

$$
\pi_i = \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}}
$$
To test this type of analysis, we are going to examine the data from:

> Long DS, Hou W, Taylor RS, McCowan LME (2016) [Serum levels of endothelial glycocalyx constituents in women at 20 weeks' gestation who later develop gestational diabetes mellitus compared to matched controls: a pilot study](https://doi.org/10.1136/bmjopen-2016-011244). BMJ Open 6: e011244. 
 
The Authors show that serum concentration of certain chemicals could predict the onset of Gestational Diabetes Mellitus (GDM). Load the data:

```{r}
long <- read_csv("data/Long_2016.csv")
head(long)
```

```{r}
logistic <- glm(data = long, formula = GDM ~ Ha, family = "binomial")
summary(logistic)
```

In this case the model cannot explain much: let's compute what is the differential probability for those with and without GDM.

```{r}
long$predictor <- logistic$fitted.values
long <- long %>% mutate(prob = exp(predictor) / (1 + exp(predictor)))
long %>% ggplot() + aes(x = factor(GDM), y = prob) + geom_boxplot()
```

## Probit regression

A similar model often used in econometric is the *probit*, which uses the c.d.f. of a normal distribution as the *link function*. In practice, the difference with the logistic regression is small. 

```{r}
probit <- glm(data = long, formula = GDM ~ Ha, family = binomial(link = "probit"))
summary(probit)
```



# Count data
## Poisson regression

Suppose your response variables are nonnegative integers. For example, we are counting the number of eggs females lay as a function of their age, body size, etc. A possible model for this case is to think of the response variable as being sampled from a Poisson distribution:

$$
Y_i \sim \text{Pois}(\lambda_i)
$$
and that the logarithm of the parameter $\lambda_i$ depends linearly on the predictors:

$$
\mathbb E[\lambda_i] = \mathbb E[\log(Y_i|X)] = \beta_0 + \beta_1 X_i
$$

In this case, our *link function* is the logarithm, transforming the relationship between the fitted values and the predictors into a linear regression. 

For a simple example, we look at the effect of wearing seatbelts on the number of drivers killed in accidents. Load the data:
```{r}
data("Seatbelts")
seat <- as.data.frame(Seatbelts)
```

and plot the histograms for before and after the law:
```{r}
library(tidyverse)
ggplot(data = seat) + aes(DriversKilled, fill = law, group = law) + geom_histogram(alpha = 0.5)
```

Now let's perform a Poisson Regression using the predictor `law` to determine two different distributions:

```{r}
pois_reg <- glm(formula = DriversKilled ~ law, data = seat, family = "poisson")
summary(pois_reg)
```

Suggesting that the law significantly decreased deaths. The AIC number at the bottom (as we will see later in the class) is a measure of goodness of fit: the lower the number, the better the fit.

## Underdispersed and Overdispersed data

The main feature of the Poisson distribution is that the mean and the variance are both equal to $\lambda$. You might remember (Taylow expansion):

$$
e^x = \sum_{n = 0}^{\infty} \frac{x^n}{n!}
$$

Then, for $X$ sampled from a Poisson distribution:

$$
\begin{aligned}
\mathbb E[X] &= \sum_{x = 0}^{\infty} x P(X = x) \\
&= \sum_{x = 0}^{\infty} x e^{-\lambda} \frac{\lambda^x}{x!} \\
&= \lambda e^{-\lambda} \sum_{(x - 1) = 0}^{\infty} \frac{\lambda^{(x-1)}}{(x-1)!} \\
&= \lambda e^{-\lambda}e^{\lambda} \\
&= \lambda
\end{aligned}
$$

Similarly,
$$
\begin{aligned}
\mathbb V[X] &= \mathbb E[X^2]-\mathbb E[X]^2\\
&= \left(\sum_{x = 0}^{\infty} x^2 e^{-\lambda} \frac{\lambda^x}{x!} \right) - \lambda^2 \\
&= \left(\lambda e^{-\lambda} \sum_{(x - 1) = 0}^{\infty} x \frac{\lambda^{(x-1)}}{(x-1)!}\right) - \lambda^2 \\
&= \left(\lambda e^{-\lambda} \left(\sum_{(x - 1) = 0}^{\infty} (x-1) \frac{\lambda^{(x-1)}}{(x-1)!} + \sum_{(x - 1) = 0}^{\infty}  \frac{\lambda^{(x-1)}}{(x-1)!} \right) \right) - \lambda^2 \\ 
&= \left(
  \lambda e^{-\lambda} 
  \left(
    \lambda \sum_{(x - 2) = 0}^{\infty} (x-2) \frac{\lambda^{(x-2)}}{(x-2)!} + e^\lambda \right)\right) - \lambda^2 \\ 
&= \left(\lambda e^{-\lambda} (\lambda e^\lambda + e^\lambda) \right) - \lambda^2\\
&= \lambda
\end{aligned}
$$

But do the data comply with this property? Let's find out:

```{r}
seat %>% group_by(law) %>% summarise(avg_deaths = mean(DriversKilled), sd_deaths = var(DriversKilled))
```

Not really. We have *overdispersed* data (i.e., the variance is much larger than what assumed by Poisson). This happens very often, and the main tool to use is a *Negative Binomial Regression* (a negative binomial distribution can be thought of as a Poisson with a scaled variance):

```{r}
# the function is contained in the MASS package
nb_reg <- MASS::glm.nb(formula = DriversKilled ~ law, data = seat)
summary(nb_reg)
```

Another option is a "quasipoisson" family:
```{r}
qpois_reg <- glm(formula = DriversKilled ~ law, data = seat, family = "quasipoisson")
summary(qpois_reg)
```

## Separate distribution for the zeros

In several cases, we have an excess of zeros. For example, you might have animals, that, if they reach the age of 1, will go on to a live a number of years well-described by say a Poisson distribution. However, mortality immediately after birth is high. In such cases, you can use zero-inflated or zero-hurdle models. 

In zero-inflated models, you can think of having a conditional branching: with probability $p_z$ your count is zero; if not (prob. $1-p_z$) it is sampled from a given distribution. As such a count of zero can stem from two different processes: either because you got a zero at the first step, or because you have sampled a zero from the distribution.

Zero-hurdle models are slightly different: you first decide whether you're going to have a zero; if not, you sample your data from a truncated distribution, such that you cannot sample zero.

To test this type of models, we're going to use data from:

> Costanzo A, Ambrosini R, Caprioli M, Gatti E, Parolini M, Canova L, Rubolini D, Romano A, Gianfranceschi L, Saino N (2017) [Lifetime reproductive success, selection on lifespan, and multiple sexual ornaments in male European barn swallows](https://doi.org/10.1111/evo.13312). Evolution 71(10): 2457-2468. 

```{r}
# Original URL
# https://datadryad.org/bitstream/handle/10255/dryad.151697/evolution_data_17-0084.R2.xls
dt <- read_xls_from_url("https://tinyurl.com/y8kvkwjp")
# select only interesting cols
dt <- dt %>% select(ID_male, tot_offspring, tail_l)
# remove NAs
dt <- dt %>% filter(!is.na(tot_offspring), !is.na(tail_l))
```

Let's take a look:
```{r}
dt %>% ggplot() + aes(x = tot_offspring) + geom_histogram()
```

You can clearly see the excess of zeros. Let's start with a Poisson, and work our way up:
```{r}
# Poisson
model1 <- glm(data = dt, tot_offspring ~ tail_l, family = "poisson")
model1
# Negative Binomial
model2 <- MASS::glm.nb(data = dt, tot_offspring ~ tail_l)
model2
# Zero-inflated Poisson
model3 <- zeroinfl(data = dt, tot_offspring ~ tail_l, dist = "pois")
model3
AIC(model3)
# Zero-inflated Negative Binomial
model4 <- zeroinfl(data = dt, tot_offspring ~ tail_l, dist = "negbin")
model4
AIC(model4)
# Zero-hurdle Poisson
model5 <- hurdle(data = dt, tot_offspring ~ tail_l, dist = "pois")
model5
AIC(model5)
# Zero-hurdle Negative Binomial
model6 <- hurdle(data = dt, tot_offspring ~ tail_l, dist = "negbin")
model6
AIC(model6)
```

# Readings

- [Regression Models for Count Data in R](https://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf)
