---
title: "Basic data wrangling"
author: "Dmitry Kondrashov & Stefano Allesina"
date: "Fundamentals of Biological Data Analysis -- BIOS 26318"
output:
  html_document:
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
urlcolor: blue
---

```{r knitr, echo=FALSE}
knitr::opts_chunk$set(
  eval      = TRUE,
  comment   = "#",
  warning = FALSE,
  message = FALSE,
  #results   = "asis",
  # collapse  = TRUE,
  fig.align = "center")
```

# Goal
Learn how to manipulate large data sets by writing efficient, consistent, and compact code. Introduce the use of `dplyr`, `tidyr`, and the "pipeline" operator `%>%`. Effortlessly produce statistics for grouped data. Massage data into "tidy" form.

# What is data wrangling?

As biologists living in the XXI century, we are often faced with tons of data, possibly replicated over several organisms, treatments, or locations. We would like to streamline and automate our analysis as much as possible, writing scripts that are easy to read, fast to run, and easy to debug. Base `R` can get the job done, but often the code contains complicated operations, and a lot of `$` signs and brackets.

We're going to learn about the packages `dplyr` and `tidyr`, which are part of `tidyverse` and can be used to manipulate large data frames in a simple and straightforward way. These tools are also much faster than the corresponding base `R` commands, are very compact, and can be concatenated into "pipelines". 

To start, we need to import the libraries:

```{r, message=FALSE}
library(tidyverse) # this loads them both, along with other packages
```

Then, we need a dataset to play with. We take a simple dataset from:

> Lippens C, Faivre B, Lechenault C, Sorci G (2017) [Aging parasites produce offspring with poor fitness prospects](https://doi.org/10.1098/rsbl.2016.0888). Biology Letters 13(2): 20160888. 

The Authors have found that senescing parasites produce offspring with lower survival probability and lower fertility. To produce this result, they have infected mice with the parasite nematode *Heligmosomoides polygyrus*. They collected the eggs found in the faeces at different times, corresponding to the age of the worm (in days). Then they used the hatched larvae to infect mice, and counted the eggs shed by the newly infected mice for 28 days. At that point, mice were sacrificed and adult worms in the intestine counted.

```{r}
# to read Excel files
source("general_code/read_xls_from_url.R")
# original URL:
# https://datadryad.org/bitstream/handle/10255/dryad.135877/dryad_data.xlsx
dt <- read_xlsx_from_url("https://tinyurl.com/yclwadb8") 
```

# A new data type, `tibble`

This is now a :
```{r}
class(dt)
```

`dplyr` ships with a new data type, called a `tibble`. To convert a `data.frame` into a tibble, use `as.tibble`:

```{r, eval=FALSE}
# load a data frame
data("trees")
class(trees)
trees <- as_tibble(trees)
class(trees)
```

The nice feature of `tbl` objects is that they will print only what fits on the screen, and also give you useful information on the size of the data, as well as the type of data in each column. Other than that, a `tbl` object behaves very much like a `data.frame`. In some rare cases, you want to transform the `tbl` back into a `data.frame`. For this, use the function `as.data.frame(tbl_object)`.

We can take a look at the data using one of several functions:

* `head(dt)` shows the first few rows
* `tail(dt)` shows the last few rows
* `glimpse(dt)` a summary of the data (similar to `str` in base R)
* `View(dt)` open in spreadsheet-like window

# Selecting rows and columns

There are many ways to subset the data, either by row (subsetting the *observations*), or by column (subsetting the *variables*). For example, suppose we want to count how many rows contain data for `time post-infection` 28. When reading data using `tidyverse`, the column names are not altered (contrary to what happens using `read.csv`). As such, they might be difficult to type. No problem: simply enclose the names with back tickmarks ("`"):

```{r}
filter(dt, `time post-infection` == 28)
```

We have 20 observations. We have used the command `filter(tbl, conditions)` to select certain observations. We can combine several conditions, by listing them side by side, possibly using logical operators.

> **Exercise:** what does this do?
>``
filter(dt, `Parental age` > 20, 
           `number of adult worms` > 20, 
           `fecal egg count` < 10)
``

We can also select particular variables (columns) using the function `select(tbl, cols to select)`. For example, select `id` and `number of adult worms`:

```{r}
select(dt, id, `number of adult worms`)
```

How many `id`s are represented in the data set? We can use the function `distinct(tbl, cols to select)` to retain only the rows that differ from each other:

```{r}
distinct(select(dt, id))
```

Showing that there are 20 replicates, once we removed the duplicates. There are many other ways to subset observations:

- `sample_n(tbl, howmany, replace = TRUE)` sample `howmany` rows at random (with replacement)
- `sample_frac(tbl, proportion, replace = FALSE)` sample a certain proportion (e.g. `0.2` for 20%) of rows at random without replacement
- `slice(tbl, 5:20)` extract the rows `5` to `20`
- ``top_n(tbl, 10, `number of adult worms`)`` extract the first `10` rows, once ordered by `number of adult worms`

More ways to select columns:

- `select(dt, contains("time"))` select all columns containing the word `time`
- ``select(dt, -id, -`Parental age`)`` exclude the columns `id` and `Parental age`
- `select(dt, matches("count|time"))` select all columns whose names match a regular expression

# Creating pipelines using `%>%`

We've been calling nested functions, such as `distinct(select(dt, id))`. If you have to add another layer or two, the code would become unreadable. `dplyr` allows you to "un-nest" these functions and create a "pipeline" in which you concatenate commands separated by a special operator, `%>%`. For example:

```{r}
dt %>% # take a data table
  select(id) %>% # select a column
  distinct() # remove duplicates
```

does exactly the same operations as the command above, but is much more readable. By concatenating many commands, you can create incredibly complex pipelines while retaining readability.

Another advantage of pipelines is that they help with name completion. In fact, `RStudio` is running in the background your pipeline while you type it. Try typing `dt %>% filter(` and then start typing `time` and press `Tab`: you will see the options to complete the column name; choose it with your arrows and hit `Return`. The back tickmarks will be added automatically.

# Producing summaries

Sometimes we need to calculate statistics on certain columns. For example, calculate the average number of eggs shedded by the infected mice. We can do this using `summarise` (you can use British or American spelling):

```{r}
dt %>% summarise(avg = mean(`fecal egg count`, na.rm = TRUE))
```

where we used `na.rm = TRUE` to ignore missing values. This command returns a `tbl` object with just the average egg count. You can combine multiple statistics (use `first`, `last`, `min`, `max`, `n` [count the number of rows], `n_distinct` [count the number of distinct rows], `mean`, `median`, `var`, `sd`, etc.):

```{r}
dt %>% summarise(avg = mean(`fecal egg count`, na.rm = TRUE), 
                 sd = sd(`fecal egg count`, na.rm = TRUE), 
                 median = median(`fecal egg count`, na.rm = TRUE))
```

# Summaries by group

One of the most useful features of `dplyr` is the ability to produce statistics for the data once subsetted by *groups*. For example, we would like to measure whether older worms produce less virulent offspring. We can then group the data by `Parental age`, and calculate the mean `fecal egg count` once the data are split into groups:

```{r}
dt %>% group_by(`Parental age`) %>% 
  summarise(mean = mean(`fecal egg count`, na.rm = TRUE))
```

showing that younger worms produce highly virulent offspring.

> **Exercise:** find the average `number of adult worms` by `Parental age`. Filter the data to consider only 28 days for `time post-infection`.   

# Ordering the data 

To order the data according to one or more variables, use `arrange()`:

```{r}
dt %>% arrange(`number of adult worms`)
dt %>% arrange(desc(`number of adult worms`))
```

# Renaming columns

To rename one or more columns, use `rename()`:
```{r}
dt %>% rename(pa = `Parental age`)
```

# Adding new variables using mutate

If you want to add one or more new columns, with the content being a function of other columns, use the function `mutate`. For example, we are going to take the samples at `time post-infection` 28, and add a column containing the ratio between `fecal egg count` and `number of adult worms` , calling the new column `worms_per_egg`:

```{r}
dt %>% 
  filter(`time post-infection` == 28) %>% 
  mutate(worms_per_egg = `fecal egg count` / `number of adult worms`)
```

We can pipe the results to `ggplot` for plotting!
```{r}
dt %>% 
  filter(`time post-infection` == 28) %>% 
  mutate(worms_per_egg = `fecal egg count` / `number of adult worms`) %>% 
  ggplot() + aes(x = `Parental age`, y = worms_per_egg) + geom_col(position = "dodge")
```

You can use the function `transmute()` to create a new column and drop the original columns. 

Most importantly, you can use `mutate` and `transmute` on grouped data. For example, let's compute a z-score of the `fecal egg count` once the data is grouped by parental age:

```{r, eval=FALSE}
dt %>% select(`Parental age`, `time post-infection`, `fecal egg count`) %>% 
  group_by(`Parental age`) %>% 
  mutate(zscore = scale(`fecal egg count`)) %>% 
  arrange(`time post-infection`)
```

# Data wrangling

Data is rarely in a format that is good for computing, and much effort goes into reading the data and wrestling with it to make it into a good format. As the name implies, `tidyverse` strongly advocates for the use of data in *tidy* form. What does this mean?

- Each variable forms a column
- Each observation forms a row
- Each type of observational unit forms a table

This is often called *narrow table* format. Any other form of data (e.g., *wide table* format) is considered *messy*. However, often data are not organized in tidy form, or we want to produce tables for human consumption rather than computer consumption. The package `tidyr` allows to accomplish just that. It contains only a few, very powerful functions:

## Gather: from columns to rows

```{r}
test <- tibble("indivudual" = c("ind1", "ind2"), "Task A" = c(1, 3), "Task B" = c(4, 1))
test
```
Make it into tidy form:

```{r}
test %>% gather(Task, Score, 2:3)
```

## Spread: turn rows into columns

```{r}
test <- tibble(location = c("one", "one", "two", "two"), 
               month = c("June", "July", "June", "July"), 
               rainfall = c(12, 14, 6, 8))
test
```
Make it into wide-table (messy) format:

```{r}
test %>% spread(month, rainfall)
```

## Separate: split a column into two or more

```{r}
test <- tibble(name = c("Allesina, Stefano", "Kondrashov, Dmitry", "Miller, Zach"))
test
```

```{r}
test %>% separate(name, into = c("last_name", "first_name"), sep = ", ")
```

The complement of `separate` is called `unite`.

## Separate rows: from one row to many

```{r}
test <- tibble(id = c(1, 2, 3, 4), records = c("a;b;c", "c;d", "a;e", "f"))
test
```

To make it into tidy form, only one record per row:

```{r}
test %>% separate_rows(records, sep = ";")
```

# An example: from tidy to messy

The data we are using are in tidy format, but suppose you want to produce a table for your paper. For each parental age, you want to show the mean number of eggs by `time post-infection`. We know how to compute the statistics:

```{r}
for_table <- dt %>% group_by(`Parental age`, `time post-infection`) %>% summarise(eggs = mean(`fecal egg count`))
for_table
```

Now the data is in narrow (tidy) format, and we would like to produce a wide table that is better for human consumption. All we need to do is to `spread` the column `time post-infection` so that each time becomes a different column. For each combination of `Parental age` and `time post-infection`, we report the mean number of eggs:
```{r}
for_table %>% spread(`time post-infection`, eggs)
```

> **Exercise**: how do we produce a table in which the `time post-infection` is in the rows, and `Parental age` in the columns?

# A more difficult case: from (very) messy to tidy

We now are going to turn a data set that is almost impossible to use. You can read the data using:

```{r}
# Original URL
# https://datadryad.org/bitstream/handle/10255/dryad.193139/Derocles%20et%20al%20Networks.xlsx
a <- read_xlsx_from_url("https://tinyurl.com/y9oqmztr")
```

These data are taken from:

> Derocles SAP, Lunt DH, Berthe SCF, Nichols PC, Moss ED, Evans DM (2018) Climate-warming alters the structure of farmland tri-trophic ecological networks and reduces crop yield. Molecular Ecology (to appear)

The data is composed of 24 matrices (one for each experimental plot) showing the frequency of interaction between plants, aphids and parasitoids. Take a look using `View(a)`: you can see that there are several matrices in the same sheet. We would like to produce the tidy data: `plot` (number of the plot), `from` (species from), `to` (species to), `frequency` (what is reported in the matrix). First, we need to find whether plot 2, 3, etc. start:

```{r}
starting <- a %>% transmute(plot = `plot 1`, 
                            start = ifelse(grepl("plot", `plot 1`), 
                                           row_number(), NA)) %>% 
  filter(!is.na(start))
```

Now let's add 1 to the starting point (because that is the row containing the names), and add at the front a new row for plot 1:
```{r}
starting <- rbind(tibble(plot = "plot 1", start = 1), 
                  starting %>% mutate(start = start + 1))
starting
```

Now we want to find where each data sets end. It will be the starting point of the next one minus 3:

```{r}
starting <- starting %>% mutate(end = lead(start) - 3)
starting
```

Note that we're missing the end for the last plot:

```{r}
tail(starting)
```

We can fix this easily:

```{r}
starting[starting$plot == "plot 24", "end"] <- nrow(a)
tail(starting)
```

Wonderful! Now let's try to transform the first plot:

```{r}
my_plot <- "plot 1"
my_start  <- 1
my_end <- 14
# build this line by line!
a %>% slice(my_start:my_end) %>% 
  gather(to, frequency, -`plot 1`) %>% 
  rename(from = `plot 1`) %>% 
  add_column(plot = my_plot) %>% 
  mutate(frequency = as.numeric(frequency)) %>% 
  filter(frequency > 0) %>% 
  select(plot, from, to, frequency)
```

Now let's put this all together!

```{r}
my_data <- tibble()
for (i in 1:nrow(starting)){
  my_plot <- starting$plot[i]
  my_start  <- starting$start[i]
  my_end <- starting$end[i]
  # build this line by line!
  my_data <- rbind(my_data,
    a %>% slice(my_start:my_end) %>% 
    gather(to, frequency, -`plot 1`) %>% 
    rename(from = `plot 1`) %>% 
    add_column(plot = my_plot) %>% 
    mutate(frequency = as.numeric(frequency)) %>% 
    filter(frequency > 0) %>% 
    select(plot, from, to, frequency))
}
```

Now it is easy to compute statistics on the data set. For example:

```{r}
my_data %>% 
  group_by(from, to) %>% 
  summarise(num = n(), 
            mean = mean(frequency), 
            sd = sd(frequency)) %>% 
  arrange(desc(mean))
```

This was a lot of work! Now you should appreciate why you need to think about how to organize your data such that they are easy to use!!

> **Exercise**: write code to build a matrix for each plot (i.e., that takes the tidy data and builds the tables such as in the original).

# Resources

* [R for Data Science](https://hackr.io/tutorial/r-for-data-science)
* A [cool class](https://cfss.uchicago.edu/syllabus.html) at U of C in Social Sciences 
* [Data transformation](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf) cheat sheet
* [Dealing with dates](https://github.com/rstudio/cheatsheets/raw/master/lubridate.pdf) cheat sheet
* [Data import](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf) cheat sheet

